# This code is part of the EMViT-DDPM project and was authored by Victor Barreiro.


"""
This script evaluates the quality of synthetic hyperspectral images generated by a diffusion model.
It calculates FrÃ©chet Inception Distance (FID), Precision, and Recall metrics by comparing
a pool of generated images against a real dataset.

The script performs the following steps:
1.  Parses command-line arguments to configure the evaluation (e.g., model path, dataset, generation parameters).
2.  Loads the real hyperspectral dataset and the pre-trained diffusion model.
3.  Sets up an experiment folder to save results and logs.
4.  Generates a set of synthetic image examples for visual inspection.
5.  Creates a larger pool of synthetic images for quantitative evaluation.
6.  Optionally normalizes the synthetic data to match the statistical properties (mean, std) of the real data.
7.  If a model for FID is provided, it calculates the FID score between real and synthetic samples.
8.  Calculates k-NN based Precision and Recall to assess the fidelity and diversity of the generated samples.
9.  Logs all configurations, results, and statistics to a file.
"""

##########################################################################################
# ------------------------------------ GLOBAL IMPORTS ----------------------------------#
##########################################################################################


import argparse
import os
import random
import time

import numpy as np
import torch
from tqdm import tqdm

import dnnlib
from gen_images import generate_images
from manifold_metrics import HYPERPARAMS, CNN2D_Residual, DatasetMock, calculate_fid, compute_precision_recall
from multispectral_utils import build_dataset, init_dataset_kwargs


def get_device():
    """Returns the available device (GPU or CPU)."""
    return torch.device("cuda:0" if torch.cuda.is_available() else "cpu")


def ensure_reproducibility(seed):
    """Sets the random seed for reproducibility and configures PyTorch for deterministic behavior."""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

    if cuda == False:
        torch.use_deterministic_algorithms(True)
        g = torch.Generator()
        g.manual_seed(SEED)
    else:
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False


def one_hot_to_index(one_hot):
    """Convert one-hot encoded labels to integer indices."""
    return int(np.argmax(one_hot))


def get_train_samples(dataset, sampling, experiment_index, num_experiments):

    rev_label_map = dataset.get_rev_label_map()

    # Set a counter per class
    counter = dict()
    for class_idx in rev_label_map.keys():
        counter[class_idx] = sampling

    # Iterate through the dataset saving the indices of each class separately
    class_indexes = dict()
    for class_idx in rev_label_map.keys():
        class_indexes[class_idx] = []
    # Start at a random index and move through the set until the data is covered
    starting_index = np.random.randint(0, len(dataset))
    total = sum(counter.values())
    pbar = tqdm(total=total, desc=f"Experiment {experiment_index+1}/{num_experiments} - Gathering real samples")

    idx = starting_index
    while sum(counter.values()) != 0:
        label = one_hot_to_index(dataset[idx][1])

        if counter[label] > 0:
            class_indexes[label].append(idx)
            counter[label] -= 1
            pbar.update(1)

        idx = (idx + 1) % len(dataset)

    pbar.close()

    assert sum(counter.values()) == 0, f"Not all classes were filled correctly: {counter}"

    samples = []
    for class_idx in class_indexes.keys():
        new_samples = [
            torch.from_numpy(dataset[j][0]).to(torch.float32) / 127.5 - 1 for j in class_indexes[class_idx]
        ]  # do not store the label
        samples.extend(new_samples)

    return torch.stack(samples)


def obtain_samples_from_pool(pool, classes, num_samples):
    """Obtain samples from the pool for the specified classes."""
    samples = []
    for class_idx in classes:
        class_samples = pool[class_idx]
        selected_indices = np.random.choice(class_samples.shape[0], num_samples, replace=False)
        for idx in selected_indices:
            samples.append((class_samples[idx], class_idx))
    return samples


def get_synthetic_samples(pool, classes, num_samples):
    samples = []
    for class_idx in classes:
        samples_class = obtain_samples_from_pool(pool, [class_idx], num_samples)
        samples_class = [np.transpose(sample[0].cpu(), (2, 0, 1)) for sample in samples_class]
        samples.extend(samples_class)
    return torch.stack(samples)


parser = argparse.ArgumentParser(description="Hyperspectral Image Classification")

# fmt: off
parser.add_argument("--network-pkl",        help="Model to use",                                                                type=str, required=True)
parser.add_argument("--patch-size",         help="Patch size",                                                                  type=int, default=32)
parser.add_argument("--fid-model-path",     help="Model to calculate FID",                                                      type=str, default=None)
# Pool size parameter
parser.add_argument("--pool-size",          help="Pool size for the generated images",                                          type=int, default=200)
# Dataset parameters
parser.add_argument("--input-path",         help="Path to the input multispectral dataset",                                     type=str, default="./data")
parser.add_argument("--filename",           help="Base filename (without extension)",                                           type=str, required=True)
parser.add_argument("--dataset-seed",       help="Random seed for dataset splitting",                                           type=int, default=0)

parser.add_argument("--batch-size_load",    help="Batch size for data loaders",                                                 type=int, default=256)
parser.add_argument("--batch-size_gen",     help="Batch size for image generation",                                             type=int, default=64)
parser.add_argument("--batch-size-metrics", help="Batch size for metric calculation",                                           type=int, default=64)

parser.add_argument("--synthetic-norm",     help="Wheter to normalize the synthetic images or not",                             action="store_true", default=False)
parser.add_argument("--show-pool-stats",    help="Wheter to show the pool statistics or not",                                   action="store_true", default=False)
# fmt: on

args = parser.parse_args()

device = get_device()
cuda = device.type == "cuda"

SEED = 42
ensure_reproducibility(seed=SEED)


##########################################################################################
# -------------------------------------- DATA LOADING ---------------------------------- #
##########################################################################################

input_dir = os.path.join(args.input_path, args.filename)
output_dir = os.path.join(args.input_path, args.filename, "patches")

# Build datasets
datasets = dict()
for split_key in ["train", "val", "test"]:
    # Build file names for datasets zips
    dataset_seed_suffix = "" if args.dataset_seed == 0 else f"_{args.dataset_seed}"
    dataset_zip_path = os.path.join(input_dir, f"{args.filename}_{split_key}{dataset_seed_suffix}.zip")
    # Initialize dataset kwargs
    dataset_kwargs, _ = init_dataset_kwargs(data=dataset_zip_path)
    dataset_kwargs.use_label_map = True
    # Build dataset and dataloader
    datasets[split_key], _ = build_dataset(
        dataset_kwargs=dataset_kwargs,
        data_loader_kwargs=dnnlib.EasyDict(),
        batch_size=args.batch_size_load,
    )

# {<index>: <label> - 1}; <index> between 0 and num_classes - 1; <label> true label in the dataset
label_map = datasets["train"].get_label_map()


##########################################################################################
# ------------------------- EXAMPLES OF SYNTHETIC GENERATIONS -------------------------- #
##########################################################################################

class_labels = list(label_map.values())

sample_out_dir = "./out/samples"
os.makedirs(sample_out_dir, exist_ok=True)

_ = generate_images.callback(
    network_pkl=args.network_pkl,
    seeds=[0],  # use always the same seed for reproducibility
    truncation_psi=1.0,
    noise_mode="const",
    outdir=sample_out_dir,
    translate=(0.0, 0.0),
    rotate=0.0,
    class_idx=None,  # no used
    classes=class_labels,
    num_images_per_class=10,
    save_images=True,
    no_rgb=False,
    no_int8=False,
    batch_size=args.batch_size_gen,
)

##########################################################################################
# ---------------------------------- IMAGE POOL GENERATION ----------------------------- #
##########################################################################################

samples = generate_images.callback(
    network_pkl=args.network_pkl,
    seeds=[SEED],  # use always the same seed for reproducibility
    truncation_psi=1.0,
    noise_mode="const",
    outdir=sample_out_dir,  # no used as save_images is False
    translate=(0.0, 0.0),
    rotate=0.0,
    class_idx=None,  # no used
    classes=class_labels,
    num_images_per_class=args.pool_size,
    save_images=False,
    no_rgb=True,
    no_int8=True,
    batch_size=args.batch_size_gen,
)

# Save the samples in a dictionary, one key for each class
pool = dict()
for i, class_idx in enumerate(class_labels):
    pool[class_idx] = samples[i * args.pool_size : (i + 1) * args.pool_size, :, :, :]

if args.show_pool_stats or args.synthetic_norm:
    # Compute pools stats, i.e., mean and std of the pool
    pool_mean = samples.mean().item()
    pool_var = samples.var().item()
    pool_std = np.sqrt(pool_var)
    print("-" * 20 + "\nPool stats:")
    print(f"Mean: {pool_mean:.6f}")
    print(f"Std: {pool_std:.6f}")

    # Compute real dataset stats, i.e., mean and std of the real dataset
    means = {"train": 0, "test": 0}
    vars = {"train": 0, "test": 0}
    stds = {"train": 0, "test": 0}

    for split in ["train", "test"]:
        for image, label in datasets[split]:
            image = image.astype(np.float32) / 127.5 - 1  # Normalize image to [-1, 1]
            means[split] += image.mean()
        means[split] = means[split] / len(datasets[split])
        for image, label in datasets[split]:
            image = image.astype(np.float32) / 127.5 - 1  # Normalize image to [-1, 1]
            vars[split] += (image.mean() - means[split]) ** 2
        vars[split] = vars[split] / len(datasets[split])
        stds[split] = np.sqrt(vars[split])
        print("-" * 20 + f"\n{split.capitalize()} set stats:")
        print(f"Mean: {means[split]:.6f}")
        print(f"Std: {stds[split]:.6f}")

    scale_factor = stds["train"] / pool_std
    shift = pool_mean - means["train"]
    print("-" * 20 + f"\nShift: {shift:.6f}")
    print(f"Scale factor: {scale_factor:.6f}")

    if args.synthetic_norm:
        # Normalize the image pool
        for class_idx in pool.keys():
            pool[class_idx] = (pool[class_idx] - shift) * scale_factor

        # Concatenate all samples to compute the new stats
        normalized_samples = torch.cat(list(pool.values()), dim=0)
        pool_mean = normalized_samples.mean().item()
        pool_var = normalized_samples.var().item()
        pool_std = np.sqrt(pool_var)
        print("Pool NORMALIZED stats")
        print("Pool stats:")
        print(f"Mean: {pool_mean:.6f}")
        print(f"Std: {pool_std:.6f}")


##########################################################################################
# --------------------------------- LOAD JUDGE MODEL ----------------------------------- #
##########################################################################################

# Load the serialized torch object (judge model)
judge_model = CNN2D_Residual(DatasetMock(), device, HYPERPARAMS)
judge_model.load_state_dict(torch.load(args.fid_model_path, map_location=device))
judge_model.to(device)
judge_model.eval()

num_gpus = torch.cuda.device_count()
if num_gpus > 1:
    print(f"Using {num_gpus} GPUs for metric calculation.")
    judge_model = torch.nn.DataParallel(judge_model)

##########################################################################################
# ------------------------------------ COMPUTE FID ------------------------------------- #
##########################################################################################


print("-" * 20 + "\nCalculating FID")

sampling_fid = args.pool_size // 2
assert (
    sampling_fid <= args.pool_size
), f"Sampling ({sampling_fid}) must be less or equal than pool size ({args.pool_size})"

experiments_fid = 5

fid_results = []
fid_times = []

for i in range(experiments_fid):
    start = time.time()

    examples_1 = get_train_samples(datasets["train"], sampling_fid, i, experiments_fid)
    examples_2 = get_synthetic_samples(pool, class_labels, sampling_fid)

    fid_results.append(
        calculate_fid(judge_model, examples_1, examples_2, batch_size=args.batch_size_metrics, device=device)
    )
    fid_times.append(time.time() - start)


##########################################################################################
# ---------------------------- COMPUTE PRECISION AND RECALL -----------------------------#
##########################################################################################


print("-" * 20 + "\nCalculating Precision and Recall")

sampling_pr = args.pool_size
assert (
    sampling_pr <= args.pool_size
), f"Sampling ({sampling_pr}) must be less or equal than pool size ({args.pool_size})"

experiments_pr = 5

times_mean = []
times_std = []

precision_results = []
recall_results = []
precision_recall_times = []

for i in range(experiments_pr):
    start = time.time()

    examples_1 = get_train_samples(datasets["train"], sampling_pr, i, experiments_pr)
    examples_2 = get_synthetic_samples(pool, class_labels, sampling_pr)

    # Extract features using the judge model
    features_1 = []
    features_2 = []

    precision, recall = compute_precision_recall(
        judge_model, examples_1, examples_2, batch_size=args.batch_size_metrics, device=device, num_gpus=num_gpus
    )

    precision_results.append(precision)
    recall_results.append(recall)
    precision_recall_times.append(time.time() - start)

##########################################################################################
# ------------------------------------- SHOW RESULTS ----------------------------------- #
##########################################################################################

# Show the results
print("-" * 20 + "\nFinal results:")
print(f"FID: {np.mean(fid_results):.6f} +/- {np.std(fid_results):.6f}")
print(f"Time: {np.mean(fid_times):.6f} +/- {np.std(fid_times):.6f}")
print(f"Precision: {np.mean(precision_results):.6f} +/- {np.std(precision_results):.6f}")
print(f"Recall: {np.mean(recall_results):.6f} +/- {np.std(recall_results):.6f}")
print(f"Time: {np.mean(precision_recall_times):.6f} +/- {np.std(precision_recall_times):.6f}")
