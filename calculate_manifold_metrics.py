# This code is part of the EMViT-DDPM project and was authored by Victor Barreiro.


"""
This script evaluates the quality of synthetic hyperspectral images generated by a diffusion model.
It calculates FrÃ©chet Inception Distance (FID), Precision, and Recall metrics by comparing
a pool of generated images against a real dataset.

The script performs the following steps:
1.  Parses command-line arguments to configure the evaluation (e.g., model path, dataset, generation parameters).
2.  Loads the real hyperspectral dataset and the pre-trained diffusion model.
3.  Sets up an experiment folder to save results and logs.
4.  Generates a set of synthetic image examples for visual inspection.
5.  Creates a larger pool of synthetic images for quantitative evaluation.
6.  Optionally normalizes the synthetic data to match the statistical properties (mean, std) of the real data.
7.  If a model for FID is provided, it calculates the FID score between real and synthetic samples.
8.  Calculates k-NN based Precision and Recall to assess the fidelity and diversity of the generated samples.
9.  Logs all configurations, results, and statistics to a file.
"""

##########################################################################################
# ------------------------------------ GLOBAL IMPORTS ----------------------------------#
##########################################################################################


import argparse
import glob
import json
import math
import os
import random
import time
import warnings

import numpy as np
import torch
from torch.utils.data import DataLoader
from tqdm import tqdm

import dnnlib
from gen_images import generate_images
from manifold_metrics import (
    HYPERPARAMS,
    CNN2D_Residual,
    DatasetMock,
    calculate_fid,
    compute_precision_recall,
    get_synthetic_features,
    get_train_features,
)
from multispectral_utils import build_dataset, init_dataset_kwargs
from visualization_utils import extract_best_tick, read_jsonl


def get_best_tick_performance(experiment_dir, output_dir, dataset_seed=None):
    """
    Extracts the best tick performance from the experiment results.

    Args:
        experiment_dir (str): Directory containing the experiment results.
        output_dir (str): Directory containing the processing summary.
        dataset_seed (int, optional): Seed used for dataset splitting (default: None).
    Returns:
        tuple: Best tick performance data and class labels.
    """

    jsonl_data = read_jsonl(os.path.join(experiment_dir, "stats.jsonl"))

    with open(
        os.path.join(
            output_dir,
            "processing_summary"
            + (f"_{dataset_seed}" if dataset_seed is not None and dataset_seed != 0 else "")
            + ".json",
        ),
        "r",
    ) as f:
        summary = json.load(f)
    label_map = summary.get("label_map", {})
    class_labels = [int(label) for label in label_map.keys()]

    best_tick_performance = extract_best_tick(
        jsonl_data,
        class_labels,
        performance_key="avg",
        verbose=False,
        only_tick_with_pkl=True,
        network_snapshot_ticks=1,
    )

    return best_tick_performance, class_labels


def select_network_snapshot(experiment_dir, output_dir, selection_method="best_val_aa", dataset_seed=None):
    """
    Selects a network snapshot from an experiment directory based on the specified method.

    Args:
        experiment_dir (str): Directory containing the experiment results.
        output_dir (str): Directory containing the processing summary.
        selection_method (str): Method for selecting the snapshot ('best_val_aa' or 'last').
        dataset_seed (int, optional): Seed used for dataset splitting (default: None).

    Returns:
        tuple: Path to the selected network snapshot and its performance data.
    """

    best_tick_performance, class_labels = get_best_tick_performance(
        experiment_dir, output_dir, dataset_seed=dataset_seed
    )
    print(f"Class labels found: {class_labels}")

    if selection_method == "best_val_aa":
        network_pkl = os.path.join(experiment_dir, f"network-snapshot-{int(best_tick_performance['kimg']):06d}.pkl")
        print(f"Selected network based on best validation AA: {network_pkl}")
        print(
            f"Best tick: {int(best_tick_performance['tick'])} with AA: {best_tick_performance['avg_accuracy_val']:.4f} and OA: {best_tick_performance['overall_accuracy_val']:.4f}"
        )
    elif selection_method == "last":
        pkl_files = sorted(glob.glob(os.path.join(experiment_dir, "network-snapshot-*.pkl")))
        if not pkl_files:
            raise ValueError(f"No network snapshots found in {experiment_dir}")
        network_pkl = pkl_files[-1]
        best_tick_performance = None  # No performance data for 'last' method
        print(f"Selected the last network snapshot: {network_pkl}")
    else:
        raise ValueError(f"Invalid selection method: {selection_method}. Choose 'best_val_aa' or 'last'.")

    return network_pkl, best_tick_performance


def output_csv_line(
    output_dir,
    output_filename,
    experiment_name,
    dataset_name,
    training_options,
    best_tick_kimg,
    mean_fid,
    std_fid,
    mean_precision,
    std_precision,
    mean_recall,
    std_recall,
):
    """
    Write a line to the results CSV file.

    Args:
        output_dir (str): Directory to save the CSV file
        output_filename (str): Name of the CSV file
        experiment_name (str): Directory name of the experiment so it can be traced back
        dataset_name (str): Name of the dataset
        training_options (dict): Dictionary of training options
        best_tick_kimg (float): Best tick in kimgs
        mean_fid (float): Mean FID score
        std_fid (float): Standard deviation of FID score
        mean_precision (float): Mean Precision score
        std_precision (float): Standard deviation of Precision score
        mean_recall (float): Mean Recall score
        std_recall (float): Standard deviation of Recall score
    """
    os.makedirs(output_dir, exist_ok=True)
    csv_path = os.path.join(output_dir, output_filename)

    # Check valid training options keys
    valid_keys = {"uniform_class", "disc_on_gen", "autoencoder_epochs"}
    for key in training_options.keys():
        if key not in valid_keys:
            raise ValueError(f"Invalid training option key: {key}. Valid keys are: {valid_keys}")

    # Create header if file does not exist
    if not os.path.exists(csv_path):
        with open(csv_path, "w") as f:
            headers = (
                ["experiment_name", "dataset"]
                + list(training_options.keys())
                + [
                    "best_tick_kimg",
                    "mean_fid",
                    "std_fid",
                    "mean_precision",
                    "std_precision",
                    "mean_recall",
                    "std_recall",
                ]
            )
            f.write(",".join(headers) + "\n")

    # Write data line
    with open(csv_path, "a") as f:
        values = (
            [experiment_name, dataset_name]
            + list(training_options.values())
            + [
                f"{float(best_tick_kimg):.3f}",
                f"{mean_fid:.6f}",
                f"{std_fid:.6f}",
                f"{mean_precision:.6f}",
                f"{std_precision:.6f}",
                f"{mean_recall:.6f}",
                f"{std_recall:.6f}",
            ]
        )
        f.write(",".join(map(str, values)) + "\n")


def get_train_size(split_info_path):
    with open(split_info_path, "r") as f:
        split_info = json.load(f)
    train_size = int(split_info.get("split_stats", {}).get("train_samples"))
    return train_size


def compute_autoencoder_epochs(autoencoder_kimg, train_size):
    if autoencoder_kimg is None:
        return None
    return math.ceil(autoencoder_kimg * 1000 / train_size)


def get_device():
    """Returns the available device (GPU or CPU)."""
    return torch.device("cuda:0" if torch.cuda.is_available() else "cpu")


def ensure_reproducibility(seed):
    """Sets the random seed for reproducibility and configures PyTorch for deterministic behavior."""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

    if cuda == False:
        torch.use_deterministic_algorithms(True)
        g = torch.Generator()
        g.manual_seed(SEED)
    else:
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False


def compute_dataset_stats(dataset, batch_size=256, num_workers=4, device="cpu", desc="Computing stats"):
    """
    Computes the mean and standard deviation of a dataset in a single pass
    using a DataLoader for efficiency.
    """
    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)

    sum_x = 0.0
    sum_sq_x = 0.0
    total_count = 0

    # Iterate once to compute sum and sum of squares
    for batch, _ in tqdm(loader, desc=desc):
        # Move to device for faster computation if available
        # Normalize to [-1, 1] as per the original logic
        batch = batch.to(device, dtype=torch.float32) / 127.5 - 1.0

        # We want the global mean/std of all pixels
        sum_x += torch.sum(batch)
        sum_sq_x += torch.sum(batch**2)
        total_count += batch.numel()

    # Move back to CPU for scalar calc
    sum_x = float(sum_x)
    sum_sq_x = float(sum_sq_x)

    mean = sum_x / total_count
    # Var = E[X^2] - (E[X])^2
    var = (sum_sq_x / total_count) - (mean**2)
    std = np.sqrt(max(0, var))  # max(0) to avoid negative due to float precision

    return mean, std


def apply_distribution_matching(pool, target_stats, pool_stats):
    """
    Normalizes the synthetic pool to match the target (real) dataset statistics.
    Logic based on original implementation:
    new_val = (old_val - shift) * scale_factor
    """
    target_mean, target_std = target_stats
    pool_mean, pool_std = pool_stats

    scale_factor = target_std / (pool_std + 1e-8)
    shift = pool_mean - target_mean

    print("-" * 20 + f"\nShift: {shift:.6f}")
    print(f"Scale factor: {scale_factor:.6f}")

    new_pool = {}
    for class_idx, images in pool.items():
        # Apply transformation
        new_pool[class_idx] = (images - shift) * scale_factor

    return new_pool


parser = argparse.ArgumentParser(description="Hyperspectral Image Classification")

# fmt: off
parser.add_argument("--network-pkl",        help="Model to use",                                                                type=str, default=None)
parser.add_argument("--experiment-dir",     help="Directory containing the experiment results (if network not provided)",       type=str, default=None)
parser.add_argument("--patch-size",         help="Patch size",                                                                  type=int, default=32)
parser.add_argument("--fid-model-path",     help="Model to calculate FID",                                                      type=str, default=None)
# Pool size parameter
parser.add_argument("--pool-size",          help="Pool size for the generated images",                                          type=int, default=200)
# Dataset parameters
parser.add_argument("--input-path",         help="Path to the input multispectral dataset",                                     type=str, default="./data")
parser.add_argument("--filename",           help="Base filename (without extension)",                                           type=str, required=True)
parser.add_argument("--dataset-seed",       help="Random seed for dataset splitting",                                           type=int, default=0)

parser.add_argument("--selection-method",   help="Method for selecting the snapshot ('best_val_aa' or 'last')",                 type=str, default="last", choices=["best_val_aa", "last"])

parser.add_argument("--batch-size-stats",   help="Batch size for statistics calculation",                                       type=int, default=512)
parser.add_argument("--batch-size-gen",     help="Batch size for image generation",                                             type=int, default=128)
parser.add_argument("--batch-size-feat",    help="Batch size for feature extraction",                                           type=int, default=512)
parser.add_argument("--num-workers",        help="Number of DataLoader workers",                                                type=int, default=3)

parser.add_argument("--synthetic-norm",     help="Wheter to normalize the synthetic images or not",                             action="store_true", default=False)
parser.add_argument("--show-pool-stats",    help="Wheter to show the pool statistics or not",                                   action="store_true", default=False)
parser.add_argument("--num-gpus",           help="Number of GPUs to use (default: all available)",                              type=int, default=None)

parser.add_argument("--output-csv",         help="Output CSV filename for results (default: manifold_results.csv)",             type=str, default="manifold_results.csv")
# fmt: on

args = parser.parse_args()
input_dir = os.path.join(args.input_path, args.filename)
output_dir = os.path.join(args.input_path, args.filename, "patches")

if args.network_pkl is None and args.experiment_dir is None:
    raise ValueError("Either --network-pkl or --experiment-dir must be provided.")
if args.network_pkl is not None and args.experiment_dir is not None:
    args.experiment_dir = None
    warnings.warn("Both --network-pkl and --experiment-dir are provided. The network will be used.", UserWarning)

best_tick_performance = None
if args.experiment_dir is not None:
    args.network_pkl, best_tick_performance = select_network_snapshot(
        experiment_dir=args.experiment_dir,
        output_dir=output_dir,
        selection_method=args.selection_method,
        dataset_seed=args.dataset_seed,
    )

device = get_device()
cuda = device.type == "cuda"

SEED = 42
ensure_reproducibility(seed=SEED)

if args.num_gpus is None:
    num_gpus = torch.cuda.device_count()
else:
    num_gpus = args.num_gpus

if num_gpus > torch.cuda.device_count():
    print(
        f"Warning: Requested {num_gpus} GPUs, but only {torch.cuda.device_count()} are available. Using all available GPUs."
    )
    num_gpus = torch.cuda.device_count()
else:
    print(f"Using {num_gpus} GPUs for calculations.")


##########################################################################################
# -------------------------------------- DATA LOADING ---------------------------------- #
##########################################################################################

# Build datasets
datasets = dict()
for split_key in ["train", "val", "test"]:
    # Build file names for datasets zips
    dataset_seed_suffix = "" if args.dataset_seed == 0 else f"_{args.dataset_seed}"
    dataset_zip_path = os.path.join(input_dir, f"{args.filename}_{split_key}{dataset_seed_suffix}.zip")
    # Initialize dataset kwargs
    dataset_kwargs, _ = init_dataset_kwargs(data=dataset_zip_path)
    dataset_kwargs.use_label_map = True
    # Build dataset and dataloader
    datasets[split_key], _ = build_dataset(
        dataset_kwargs=dataset_kwargs,
        data_loader_kwargs=dnnlib.EasyDict(),
        batch_size=args.batch_size_feat,
    )

# {<index>: <label> - 1}; <index> between 0 and num_classes - 1; <label> true label in the dataset
label_map = datasets["train"].get_label_map()


##########################################################################################
# ------------------------- EXAMPLES OF SYNTHETIC GENERATIONS -------------------------- #
##########################################################################################

class_labels = list(label_map.values())

sample_out_dir = "./out/samples"
os.makedirs(sample_out_dir, exist_ok=True)

_ = generate_images.callback(
    network_pkl=args.network_pkl,
    seeds=[0],  # use always the same seed for reproducibility
    truncation_psi=1.0,
    noise_mode="const",
    outdir=sample_out_dir,
    translate=(0.0, 0.0),
    rotate=0.0,
    class_idx=None,  # no used
    classes=class_labels,
    num_images_per_class=10,
    save_images=True,
    no_rgb=False,
    no_int8=False,
    batch_size=args.batch_size_gen,
    num_gpus=num_gpus,
)

##########################################################################################
# ---------------------------------- IMAGE POOL GENERATION ----------------------------- #
##########################################################################################

samples = generate_images.callback(
    network_pkl=args.network_pkl,
    seeds=[SEED],  # use always the same seed for reproducibility
    truncation_psi=1.0,
    noise_mode="const",
    outdir=sample_out_dir,  # no used as save_images is False
    translate=(0.0, 0.0),
    rotate=0.0,
    class_idx=None,  # no used
    classes=class_labels,
    num_images_per_class=args.pool_size,
    save_images=False,
    no_rgb=True,
    no_int8=True,
    batch_size=args.batch_size_gen,
    num_gpus=num_gpus,
)

# Save the samples in a dictionary, one key for each class
pool = dict()
for i, class_idx in enumerate(class_labels):
    pool[class_idx] = samples[i * args.pool_size : (i + 1) * args.pool_size, :, :, :]

if args.show_pool_stats or args.synthetic_norm:
    # Compute pools stats
    pool_mean = samples.mean().item()
    pool_std = samples.std().item()

    print("-" * 20 + "\nPool stats:")
    print(f"Mean: {pool_mean:.6f}")
    print(f"Std: {pool_std:.6f}")

    # Compute real dataset stats efficiently
    # We use num_workers for parallel loading and calculate in one pass
    real_stats = {}
    for split in ["train", "test"]:
        mean, std = compute_dataset_stats(
            datasets[split],
            batch_size=args.batch_size_stats,
            device=device,
            desc=f"Stats ({split})",
        )
        real_stats[split] = (mean, std)

        print("-" * 20 + f"\n{split.capitalize()} set stats:")
        print(f"Mean: {mean:.6f}")
        print(f"Std: {std:.6f}")

    if args.synthetic_norm:
        # Normalize the image pool using Train stats
        pool = apply_distribution_matching(pool, target_stats=real_stats["train"], pool_stats=(pool_mean, pool_std))

        # Re-compute stats for verification
        normalized_samples = torch.cat(list(pool.values()), dim=0)
        pool_mean = normalized_samples.mean().item()
        pool_std = normalized_samples.std().item()

        print("Pool NORMALIZED stats")
        print("Pool stats:")
        print(f"Mean: {pool_mean:.6f}")
        print(f"Std: {pool_std:.6f}")


##########################################################################################
# --------------------------------- LOAD JUDGE MODEL ----------------------------------- #
##########################################################################################

# Load the serialized torch object (judge model)
judge_model = CNN2D_Residual(DatasetMock(), device, HYPERPARAMS)
judge_model.load_state_dict(torch.load(args.fid_model_path, map_location=device))
judge_model.to(device)
judge_model.eval()

if num_gpus > 1:
    print(f"Using {num_gpus} GPUs for metric calculation.")
    judge_model = torch.nn.DataParallel(judge_model)

##########################################################################################
# ------------------------------------ COMPUTE FID ------------------------------------- #
##########################################################################################


print("-" * 20)

sampling_fid = args.pool_size

experiments_fid = 5

fid_results = []
fid_times = []

for i in tqdm(range(experiments_fid), desc="FID Experiments"):
    start = time.time()

    features_1 = get_train_features(
        datasets["train"],
        sampling_fid,
        judge_model,
        batch_size=args.batch_size_feat,
        num_workers=args.num_workers,
        device=device,
    )
    features_2 = get_synthetic_features(
        pool,
        class_labels,
        sampling_fid,
        judge_model,
        batch_size=args.batch_size_feat,
        device=device,
    )

    fid_results.append(calculate_fid(features_1, features_2))
    fid_times.append(time.time() - start)


##########################################################################################
# ---------------------------- COMPUTE PRECISION AND RECALL -----------------------------#
##########################################################################################


print("-" * 20)

sampling_pr = args.pool_size
assert (
    sampling_pr <= args.pool_size
), f"Sampling ({sampling_pr}) must be less or equal than pool size ({args.pool_size})"

experiments_pr = 5

times_mean = []
times_std = []

precision_results = []
recall_results = []
precision_recall_times = []

for i in tqdm(range(experiments_pr), desc="Precision/Recall Experiments"):
    start = time.time()

    features_1 = get_train_features(
        datasets["train"],
        sampling_pr,
        judge_model,
        batch_size=args.batch_size_feat,
        num_workers=args.num_workers,
        device=device,
    )
    features_2 = get_synthetic_features(
        pool,
        class_labels,
        sampling_pr,
        judge_model,
        batch_size=args.batch_size_feat,
        device=device,
    )

    precision, recall = compute_precision_recall(features_1, features_2, num_gpus=num_gpus)

    precision_results.append(precision)
    recall_results.append(recall)
    precision_recall_times.append(time.time() - start)

##########################################################################################
# ------------------------------------- SHOW RESULTS ----------------------------------- #
##########################################################################################

mean_fid, std_fid = np.mean(fid_results), np.std(fid_results)
mean_fid_time, std_fid_time = np.mean(fid_times), np.std(fid_times)

mean_precision, std_precision = np.mean(precision_results), np.std(precision_results)
mean_recall, std_recall = np.mean(recall_results), np.std(recall_results)
mean_pr_time, std_pr_time = np.mean(precision_recall_times), np.std(precision_recall_times)

# Output CSV line
if args.output_csv:
    if args.experiment_dir is None or best_tick_performance is None:
        # Get the dir from the network path
        args.experiment_dir = os.path.dirname(args.network_pkl)
        best_tick_performance, _ = get_best_tick_performance(args.experiment_dir, output_dir)

    train_size = get_train_size(
        os.path.join(
            input_dir,
            "patches",
            "split_info"
            + (f"_{args.dataset_seed}" if args.dataset_seed is not None and args.dataset_seed != 0 else "")
            + ".json",
        )
    )
    with open(os.path.join(args.experiment_dir, "training_options.json"), "r") as f:
        training_options = json.load(f)

    training_options = {
        "uniform_class": training_options.get("uniform_class_labels", None),
        "disc_on_gen": training_options.get("disc_on_gen", None),
        "autoencoder_epochs": compute_autoencoder_epochs(training_options.get("autoencoder_kimg", None), train_size),
    }

    output_csv_line(
        output_dir=".",
        output_filename=args.output_csv,
        experiment_name=args.experiment_dir,
        dataset_name=args.filename,
        training_options=training_options,
        best_tick_kimg=best_tick_performance["kimg"],
        mean_fid=mean_fid,
        std_fid=std_fid,
        mean_precision=mean_precision,
        std_precision=std_precision,
        mean_recall=mean_recall,
        std_recall=std_recall,
    )
    print(f"Results written to {os.path.join('.', args.output_csv)}")


# Show the results
print("-" * 20 + "\nFinal results:")
print(f"FID: {mean_fid:.6f} +/- {std_fid:.6f}")
print(f"Time: {mean_fid_time:.6f} +/- {std_fid_time:.6f}")
print(f"Precision: {mean_precision:.6f} +/- {std_precision:.6f}")
print(f"Recall: {mean_recall:.6f} +/- {std_recall:.6f}")
print(f"Time: {mean_pr_time:.6f} +/- {std_pr_time:.6f}")
